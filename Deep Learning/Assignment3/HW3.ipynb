{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sample in batch_size:\n",
    "#     padding_width = input_width + 2*padding\n",
    "#     padding_height = input_height + 2*padding\n",
    "#     # Given that output_width and output_height are the same:\n",
    "#     output_width = output_height = ((input_width - kernel_size + 2*padding)/stride) + 1\n",
    "#     # Create empty output for this layer\n",
    "#     output = zeros([total_kernels, output_width, output_height])\n",
    "#     for layer in input_channels:\n",
    "#         layer_pad = [sample, layer, :, :]\n",
    "#         # Add padding\n",
    "#         for pad in padding:\n",
    "#             add column of zeros to right to layer_pad\n",
    "#             add column of zeros to left to layer_pad\n",
    "#             add column of zeros to top to layer_pad\n",
    "#             add column of zeros to bottom to layer_pad\n",
    "#         # Move over layer:\n",
    "#         for y in range(output_height-1):\n",
    "#             for x in range(output_width-1):\n",
    "#                 # If multiple kernels exist, save in different slices of output\n",
    "#                 for kernel in total_kernels:\n",
    "#                     patch = layer_pad[y*stride : (y*stride)+kernel_size, x*stride : (x*stride)+kernel_size]\n",
    "#                     output[kernel, y, x] += sum( patch * kernel[layer, :, :] )\n",
    "#     sample_output[sample, :, :, :] = output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2. \n",
    "For a given input tensor, kernel size, stride and padding (no dilutions) work out\n",
    "a general function that computes the size of the output.\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html?highlight=functional%20conv2d#torch.nn.functional.conv2d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import ipykernel\n",
    "import math\n",
    "ipykernel.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0\n",
      "torch.Size([1, 1, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "tensor = np.random.rand(1, 2, 27, 27)\n",
    "weight = np.random.rand(1, 2, 3, 3)\n",
    "\n",
    "def output_size(input_tensor, kernel_size, stride, padding):\n",
    "    batch_size, channels, height, width = input_tensor.shape # <- similar to tensor.size() in torch, but '.shape' here since it is numpy\n",
    "    out_size = (((height - kernel_size + 2*padding) / stride ) + 1)\n",
    "    return out_size\n",
    "\n",
    "print(output_size(tensor, 3, 2, 0)) #<- example from the slides: Lecture 3 AlexNet, top right\n",
    "\n",
    "tensor = torch.rand([1, 2, 27, 27])\n",
    "weight = torch.rand([1, 2, 3, 3])\n",
    "\n",
    "print(torch.nn.functional.conv2d(tensor, weight, stride=2,padding=0).size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: \n",
    "Write a naive (non-vectorized) implementation of the unfold function in\n",
    "pseudocode. Include the pseudocode in your report.\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.functional.unfold.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #input_tensor = [b, c, h, w] #batch_size, channels, height, width\n",
    "\n",
    "# # Pseudo code naive unfold:\n",
    "# def naive_unfold(input_tensor, kernel_size, stride, padding):\n",
    "#     output_size = output_size(input_tensor, kernel_size, stride, padding)\n",
    "#     #1 extract all patches from the input\n",
    "#     for sample in b:\n",
    "#         for channel in c:\n",
    "#             layer_pad = [sample, layer, :, :]\n",
    "#             # Add padding\n",
    "#             for pad in padding:\n",
    "#                 add column of zeros to right to layer_pad\n",
    "#                 add column of zeros to left to layer_pad\n",
    "#                 add column of zeros to top to layer_pad\n",
    "#                 add column of zeros to bottom to layer_pad\n",
    "#             n_patches_per_layer = output_size * output_size\n",
    "#             for y in range(output_size-1):\n",
    "#                 for x in range(output_size-1):\n",
    "#                     # x+ y = number of patch, total_patch is 0 first time\n",
    "#                     total_patch = (x+y) * channel\n",
    "#                     patch[sample, x+y + total_patch ,:, :] = layer_pad[y*stride : (y*stride)+kernel_size, \n",
    "#                                                                         x*stride : (x*stride)+kernel_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_batch =  torch.Size([16, 3, 32, 32])\n",
      "unfolded =  torch.Size([16, 27, 1024])\n",
      "X_reshaped =  torch.Size([16384, 27])\n",
      "W =  torch.Size([27, 8])\n",
      "Y =  torch.Size([16384, 8])\n",
      "Y_reshaped =  torch.Size([16, 1024, 8])\n",
      "Y_permuted =  torch.Size([16, 8, 1024])\n",
      "output =  torch.Size([16, 8, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels, kernel_size=(3,3), stride=1, padding=1):\n",
    "        super().__init__() # <- belangrijk!\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "    def forward(self, input_batch):\n",
    "        batch_size, channels, height, width = input_batch.size()\n",
    "        print(\"input_batch = \", input_batch.size())\n",
    "        \n",
    "        # output dimensions\n",
    "        h_out = int(output_size(input_batch, self.kernel_size[0], self.stride, self.padding))\n",
    "        w_out = int(output_size(input_batch, self.kernel_size[1], self.stride, self.padding))\n",
    "        \n",
    "        # unfolded matrix (b, k, p)\n",
    "        unfolded = F.unfold(input_batch, self.kernel_size, padding=self.padding, stride=self.stride)\n",
    "        print(\"unfolded = \", unfolded.size())\n",
    "        batch_size, k_values_per_patch, patches = unfolded.size()\n",
    "        \n",
    "        # reshape to (b, p, k) tensor, than merge b and p to get (b*p,k) tensor\n",
    "        reshaped = torch.transpose(unfolded, 1, 2).reshape(-1, k_values_per_patch)\n",
    "        print(\"X_reshaped = \", reshaped.size())\n",
    "        \n",
    "        # Initiate random weights with correct dimensions\n",
    "        W = torch.rand((k_values_per_patch, self.out_channels)) # - rows: number of nodes in one patch of input. -columns: # of nodes in one pixel in output\n",
    "        print(\"W = \", W.size())\n",
    "        \n",
    "        # Matrix multiplication to get Y\n",
    "        Y = torch.mm(reshaped, W) # bmm?\n",
    "        print(\"Y = \", Y.size())\n",
    "        \n",
    "        # Reshape to get seperate batches back\n",
    "        Y_reshaped = Y.reshape((batch_size, patches, self.out_channels)) # contains one row-vector for each pixel in output\n",
    "        print(\"Y_reshaped = \", Y_reshaped.size())\n",
    "        \n",
    "        # Permute to swap axis for p and k\n",
    "        Y_permuted = torch.permute(Y_reshaped, (0, 2, 1))\n",
    "        print(\"Y_permuted = \", Y_permuted.size())\n",
    "        \n",
    "        # Fold back to obtain the output of this layer\n",
    "        output = Y_permuted.reshape(batch_size, self.out_channels, h_out, w_out)\n",
    "        print(\"output = \", output.size())\n",
    "        \n",
    "        assert output.size() == torch.nn.functional.conv2d(input_batch, W.reshape(self.out_channels, self.in_channels, self.kernel_size[0], self.kernel_size[1]),padding=self.padding).size()\n",
    "        return output\n",
    "\n",
    "# We use the Conv2D module by instantiating it, and applying it to an input.\n",
    "torch.manual_seed(0)\n",
    "conv = Conv2D(in_channels= 3, out_channels= 8)\n",
    "input_batch = torch.randn(16, 3, 32, 32)\n",
    "output_batch = conv(input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4, 5 & 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  torch.Size([16, 3, 32, 32])\n",
      "U =  torch.Size([16, 27, 1024])\n",
      "U_reshaped =  torch.Size([16384, 27])\n",
      "W =  torch.Size([27, 8])\n",
      "Y =  torch.Size([16384, 8])\n",
      "Y_reshaped =  torch.Size([16, 1024, 8])\n",
      "Y_permuted =  torch.Size([16, 8, 1024])\n",
      "output_batch =  torch.Size([16, 8, 32, 32])\n",
      "\n",
      "\n",
      "grad_Y_permuted =  torch.Size([16, 8, 1024])\n",
      "grad_Y_reshaped =  torch.Size([16, 1024, 8])\n",
      "grad_Y =  torch.Size([16384, 8])\n",
      "grad_W =  torch.Size([27, 8])\n",
      "grad_U_reshaped =  torch.Size([16384, 27])\n",
      "grad_U =  torch.Size([16, 27, 1024])\n",
      "grad_X =  torch.Size([16, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "class MyConv2DFunc(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward\n",
    "    passes which operate on Tensors.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_batch, kernel, stride=1, padding=1):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input\n",
    "        and return a Tensor containing the output. ctx is a context\n",
    "        object that can be used to stash information for backward\n",
    "        computation. You can cache arbitrary objects for use in the\n",
    "        backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        output_channels = kernel.size()[1]\n",
    "        \n",
    "        # your code here\n",
    "        batch_size, input_channels, height, width = input_batch.size()\n",
    "        print(\"X = \", input_batch.size())\n",
    "        \n",
    "        kernel_size = int(math.sqrt(kernel.size()[0]/input_channels))\n",
    "        \n",
    "        # output dimensions\n",
    "        h_out = int(output_size(input_batch, kernel_size, stride, padding))\n",
    "        w_out = int(output_size(input_batch, kernel_size, stride, padding))\n",
    "        \n",
    "        # unfolded matrix (b, k, p)\n",
    "        U = F.unfold(input_batch, (kernel_size, kernel_size), padding=padding, stride=stride)\n",
    "        print(\"U = \", U.size())\n",
    "        batch_size, k_values_per_patch, patches = U.size()\n",
    "        \n",
    "        # reshape to (b, p, k) tensor, than merge b and p to get (b*p,k) tensor\n",
    "        U_reshaped = torch.transpose(U, 1, 2).reshape(-1, k_values_per_patch)\n",
    "        print(\"U_reshaped = \", U_reshaped.size())\n",
    "        \n",
    "        # Initiate random weights with correct dimensions\n",
    "        W = torch.rand((k_values_per_patch, output_channels)) # - rows: number of nodes in one patch of input. -columns: # of nodes in one pixel in output\n",
    "        print(\"W = \", W.size())\n",
    "    \n",
    "        # store objects for the backward\n",
    "        ctx.save_for_backward(input_batch, U_reshaped, W)\n",
    "        \n",
    "        # Matrix multiplication to get Y\n",
    "        Y = torch.mm(U_reshaped, W) # bmm?\n",
    "        print(\"Y = \", Y.size())\n",
    "        \n",
    "        # Reshape to get seperate batches back\n",
    "        Y_reshaped = Y.reshape((batch_size, patches, output_channels)) # contains one row-vector for each pixel in output\n",
    "        print(\"Y_reshaped = \", Y_reshaped.size())\n",
    "        \n",
    "        # Permute to swap axis for p and k\n",
    "        Y_permuted = torch.permute(Y_reshaped, (0, 2, 1))\n",
    "        print(\"Y_permuted = \", Y_permuted.size())\n",
    "        \n",
    "        output_batch = Y_permuted.reshape(batch_size, output_channels, h_out, w_out)\n",
    "        print(\"output_batch = \", output_batch.size())\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        assert output_batch.size() == torch.nn.functional.conv2d(input_batch, W.reshape(output_channels, input_channels, kernel_size, kernel_size),padding=padding).size()\n",
    "        return output_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the\n",
    "        gradient of the loss with respect to the output, and we need\n",
    "        to compute the gradient of the loss with respect to the\n",
    "        input\n",
    "        \"\"\"\n",
    "        # retrieve stored objects\n",
    "        input_batch, X_reshaped, W = ctx.saved_tensors\n",
    "        # your code here\n",
    "        \n",
    "        grad_Y_permuted = grad_output.reshape(grad_output.size()[0], grad_output.size()[1], grad_output.size()[2]*grad_output.size()[2])\n",
    "        print(\"grad_Y_permuted = \", grad_Y_permuted.size())\n",
    "        grad_Y_reshaped = torch.permute(grad_Y_permuted, (0, 2, 1))\n",
    "        print(\"grad_Y_reshaped = \", grad_Y_reshaped.size())\n",
    "        grad_Y = grad_Y_reshaped.reshape(grad_Y_reshaped.size()[0]*grad_Y_reshaped.size()[1],grad_Y_reshaped.size()[2])\n",
    "        print(\"grad_Y = \", grad_Y.size())\n",
    "        kernel_grad = torch.transpose(torch.mm(torch.transpose(grad_Y,0,1), X_reshaped),0,1)\n",
    "        print(\"grad_W = \", kernel_grad.size())\n",
    "        grad_U_reshaped = torch.mm(grad_Y, torch.transpose(W,0,1))\n",
    "        print(\"grad_U_reshaped = \", grad_U_reshaped.size())\n",
    "        grad_U = torch.permute(grad_U_reshaped.reshape(input_batch.size()[0], int(grad_U_reshaped.size()[0]/input_batch.size()[0]), kernel_grad.size()[0]), (0, 2, 1))\n",
    "        print(\"grad_U = \", grad_U.size())\n",
    "        input_batch_grad = F.fold(grad_U, output_size=[input_batch.size()[2], input_batch.size()[3]], kernel_size=(3,3), padding=1)\n",
    "        print(\"grad_X = \", input_batch_grad.size())\n",
    "        return input_batch_grad, kernel_grad, None, None\n",
    "        \n",
    "input_channels = 3\n",
    "output_channels = 8\n",
    "kernel_size = 3\n",
    "\n",
    "input_batch = torch.randn(16, 3, 32, 32, requires_grad=True)\n",
    "kernel = torch.randn(kernel_size*kernel_size*input_channels, output_channels, requires_grad=True)\n",
    "\n",
    "conv = MyConv2DFunc.apply\n",
    "output = conv(input_batch, kernel)\n",
    "loss = output.sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "arg = {\"data\":'./data', \"batch\": 60000} # with batch = 16 we get a dataloader of length 3750 (*16=60.000)\n",
    "training_data = torchvision.datasets.MNIST(root=arg['data'], train=True, download=True, transform=ToTensor())\n",
    "trainloader = torch.utils.data.DataLoader(training_data, batch_size=arg['batch'], shuffle=True, num_workers=2)\n",
    "test_set = torchvision.datasets.MNIST(root=arg['data'], train=False, download=True, transform=ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=arg['batch'], shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(trainloader):\n",
    "    input, labels = data\n",
    "    \n",
    "training = (input[:50000])\n",
    "training_label = labels[:50000]\n",
    "validation = input[50000:]\n",
    "validation_label = labels[50000:]\n",
    "\n",
    "def loop_over(data, label, step):\n",
    "    for i in range(0,len(data),step):\n",
    "        batch = data[i:i+step]\n",
    "        batch_labels = label[i:i+step]\n",
    "\n",
    "loop_over(training, training_label, 16)\n",
    "loop_over(validation, validation_label, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "Build this network and tune the hyperparameters until you get a good baseline\n",
    "performance you are happy with. You should be able to get at least 95% accuracy. If training\n",
    "takes too long, you can reduce the number of channels in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms\n",
    "from torchvision.transforms import ToTensor, transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_chan, kernel, stride, padding, output):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(input_chan, 16, kernel, stride, padding), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(16, 32, kernel, stride, padding), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(32, 64, kernel, stride, padding), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.out = nn.Linear(64*3*3, output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(batch_size):\n",
    "    loaders = {'train_set' : DataLoader(train_set, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "               'val_set' : DataLoader(val_set, \n",
    "                                          batch_size=10000, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "                'test_set'  : DataLoader(test_set, \n",
    "                                          batch_size=len(test_set), \n",
    "                                          shuffle=False, \n",
    "                                          num_workers=1)}\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, loaders):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loaders['val_set']:\n",
    "            output = net(x)\n",
    "            _,pred_y = torch.max(output, dim = 1)\n",
    "            correct += (pred_y == y).float().sum()\n",
    "\n",
    "    print('accuracy on validation set', (correct / 10000)*100, '%')\n",
    "    return (correct / 10000)*100\n",
    "\n",
    "def train(net, loaders, epochs, loss_f, opt):\n",
    "    train_loss = []\n",
    "    epoch_list = []\n",
    "    acc_list = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print('epoch = ', i)\n",
    "        for j, (x, y) in enumerate(loaders['train_set']):\n",
    "            opt.zero_grad()\n",
    "            x_batch = x\n",
    "            y_batch = y\n",
    "            output = net.forward(x_batch)\n",
    "            loss = loss_f(output, y_batch)\n",
    "            train_loss.append(loss)\n",
    "            if j % 1000 == 0:\n",
    "                print('loss:', loss.item())\n",
    "            loss.backward()\n",
    "            opt.step() \n",
    "        epoch_list.append(i)\n",
    "        acc_list.append(validate(net, loaders))\n",
    "    return train_loss, epoch_list, acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0\n",
      "loss: 2.303809642791748\n",
      "loss: 0.4514291286468506\n",
      "loss: 0.27939900755882263\n",
      "loss: 0.13594754040241241\n",
      "accuracy on validation set tensor(94.6100) %\n",
      "epoch =  1\n",
      "loss: 0.10785152018070221\n",
      "loss: 0.05007141828536987\n",
      "loss: 0.25156405568122864\n",
      "loss: 0.22109884023666382\n",
      "accuracy on validation set tensor(96.3400) %\n",
      "epoch =  2\n",
      "loss: 0.22883853316307068\n",
      "loss: 0.15962332487106323\n",
      "loss: 0.08423075079917908\n",
      "loss: 0.23128293454647064\n",
      "accuracy on validation set tensor(97.1200) %\n",
      "epoch =  3\n",
      "loss: 0.004138742573559284\n",
      "loss: 0.014569264836609364\n",
      "loss: 0.051792778074741364\n",
      "loss: 0.009106600657105446\n",
      "accuracy on validation set tensor(96.7200) %\n",
      "epoch =  4\n",
      "loss: 0.0018573529087007046\n",
      "loss: 0.053037963807582855\n",
      "loss: 0.009400795213878155\n",
      "loss: 0.3510056138038635\n",
      "accuracy on validation set tensor(97.8300) %\n"
     ]
    }
   ],
   "source": [
    "input_chan = 1\n",
    "output = 10\n",
    "kernel = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "lr = 0.0001\n",
    "net = Net(input_chan, kernel, stride, padding, output)\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(net.parameters(), lr)\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "train_set, val_set = torch.utils.data.random_split(training_data, [50000, 10000])\n",
    "loaders = get_loaders(batch_size) \n",
    "train_loss, epoch_list, acc_list = train(net, loaders, epochs, loss_f, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = acc_list\n",
    "x1 = epoch_list\n",
    "\n",
    "plt.plot(x1, y1, label = \"batch_size = 16\" )\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy during training epochs')\n",
    "plt.xticks(np.arange(0, 20, 1))\n",
    "plt.savefig('figures/Q8_acc')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented = transforms.Compose(\n",
    "                    [\n",
    "                    transforms.RandomRotation((-7.0,7.0),fill=(1,)),\n",
    "                    # transforms.RandomAffine((-3,3),translate =(0.05,0.05)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1308,), (0.3016,)) \n",
    "                    ])\n",
    "\n",
    "training_data = torchvision.datasets.MNIST(root=arg['data'], train=True, download=True, transform=augmented)\n",
    "train_set, val_set = torch.utils.data.random_split(training_data, [50000, 10000])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0\n",
      "loss: 2.2726919651031494\n",
      "loss: 0.06206151098012924\n",
      "loss: 0.10813235491514206\n",
      "loss: 0.06566769629716873\n",
      "accuracy on validation set tensor(96.0500) %\n",
      "epoch =  1\n",
      "loss: 0.060345299541950226\n",
      "loss: 0.324048787355423\n",
      "loss: 0.2266087383031845\n",
      "loss: 0.023933807387948036\n",
      "accuracy on validation set tensor(96.8300) %\n",
      "epoch =  2\n",
      "loss: 0.019474992528557777\n",
      "loss: 0.09457053989171982\n",
      "loss: 0.009269594214856625\n",
      "loss: 0.18167942762374878\n",
      "accuracy on validation set tensor(97.6100) %\n",
      "epoch =  3\n",
      "loss: 0.017100483179092407\n",
      "loss: 0.0025716719683259726\n",
      "loss: 0.034040018916130066\n",
      "loss: 0.0017928642919287086\n",
      "accuracy on validation set tensor(98.1100) %\n",
      "epoch =  4\n",
      "loss: 0.0949849784374237\n",
      "loss: 0.3918619155883789\n",
      "loss: 0.021610355004668236\n",
      "loss: 0.0013072652509436011\n",
      "accuracy on validation set tensor(98.3600) %\n"
     ]
    }
   ],
   "source": [
    "input_chan = 1\n",
    "output = 10\n",
    "kernel = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "lr = 0.0001\n",
    "net = Net(input_chan, kernel, stride, padding, output)\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(net.parameters(), lr)\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "loaders = get_loaders(batch_size) \n",
    "train_loss, epoch_list, acc_list = train(net, loaders, epochs, loss_f, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_list, acc_list, label = \"batch_size = 16\" )\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy during training epochs')\n",
    "plt.xticks(np.arange(0, 20, 1))\n",
    "plt.savefig('Q9_acc_augmented')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 511, 383])\n",
      "torch.Size([1, 16, 959, 539])\n",
      "ERROR: Dimensions are not correct..\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.rand(1, 3, 1024, 768)\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride = 2, padding=1)\n",
    "print(conv_layer(input_tensor).size())\n",
    "\n",
    "input_tensor = torch.rand(1, 3, 1920, 1080)\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride = 2, padding=1)\n",
    "print(conv_layer(input_tensor).size())\n",
    "\n",
    "input_tensor = torch.rand(1, 8, 1920, 1080)\n",
    "try:\n",
    "    conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride = 2, padding=1)\n",
    "    print(conv_layer(input_tensor).size())\n",
    "except:\n",
    "    print(\"ERROR: Dimensions are not correct..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 1, 1])\n",
      "torch.Size([16, 3, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "b = 16\n",
    "c = 3\n",
    "h = 32\n",
    "w = 32\n",
    "\n",
    "input_tensor = torch.rand((b, c, h, w))\n",
    "global_average_pooling = torch.nn.AvgPool2d((h,w), stride=None, padding=0)\n",
    "global_max_pooling = torch.nn.MaxPool2d((h,w), stride=None, padding=0)\n",
    "avg_pooled = global_average_pooling(input_tensor)  \n",
    "max_pooled = global_max_pooling(input_tensor)\n",
    "print(avg_pooled.size())\n",
    "print(max_pooled.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_size = transforms.Compose(\n",
    "                    [\n",
    "                    transforms.Resize((28,28)),\n",
    "                    transforms.Grayscale(num_output_channels=1),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1308,), (0.3016,)) \n",
    "                    ])\n",
    "\n",
    "training_data = torchvision.datasets.ImageFolder('mnist-varres/train', transform=one_size)\n",
    "test_set= torchvision.datasets.ImageFolder('mnist-varres/test', transform=one_size)\n",
    "train_set, val_set = torch.utils.data.random_split(training_data, [50000, 10000]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(training_data, batch_size=arg['batch'], shuffle=True, num_workers=2)\n",
    "for i, data in enumerate(trainloader):\n",
    "    input, labels = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0\n",
      "loss: 2.319960594177246\n",
      "loss: 1.245522379875183\n",
      "loss: 0.9869928359985352\n",
      "loss: 0.8931272625923157\n",
      "accuracy on validation set tensor(76.1700) %\n",
      "epoch =  1\n",
      "loss: 1.0926913022994995\n",
      "loss: 1.0362154245376587\n",
      "loss: 0.49100130796432495\n",
      "loss: 0.41632211208343506\n",
      "accuracy on validation set tensor(85.2100) %\n",
      "epoch =  2\n",
      "loss: 0.39689311385154724\n",
      "loss: 0.20879657566547394\n",
      "loss: 0.376911997795105\n"
     ]
    }
   ],
   "source": [
    "input_chan = 1 \n",
    "output = 10\n",
    "kernel = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "lr = 0.0001\n",
    "net = Net(input_chan, kernel, stride, padding, output)\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(net.parameters(), lr)\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "loaders = get_loaders(batch_size) \n",
    "train_loss, epoch_list, acc_list = train(net, loaders, epochs, loss_f, opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Overleaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_chan, kernel, strd, padding, output, pool):\n",
    "        super().__init__()\n",
    "        self.pool = pool\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(input_chan, 16, kernel, strd, padding), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(16, 32, kernel, strd, padding), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(32, 81, kernel, strd, padding), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.out = nn.Linear(81, output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        if self.pool == 'avg':\n",
    "            global_avg_pooling = torch.nn.AvgPool2d((x.shape[2],x.shape[3]))\n",
    "            x = global_avg_pooling(x)\n",
    "        if self.pool == 'max':\n",
    "            global_max_pooling = torch.nn.MaxPool2d((x.shape[2],x.shape[3]))\n",
    "            x = global_max_pooling(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetFixed(nn.Module):\n",
    "    def __init__(self, input_chan, kernel, stride, padding, output):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(input_chan, 16, kernel, stride, padding), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(16, 32, kernel, stride, padding), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(32, 64, kernel, stride, padding), nn.ReLU(), nn.MaxPool2d(2,2))\n",
    "        self.out = nn.Linear(64*3*3, output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        output = self.out(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, loader):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader['val_set']: \n",
    "            output = net(x)\n",
    "            _,pred_y = torch.max(output, dim = 1)\n",
    "            correct = np.array(pred_y == y)  \n",
    "            accuracy = np.count_nonzero(correct)/len(correct)\n",
    "        print(\"Acc = \", accuracy)\n",
    "    return accuracy\n",
    "\n",
    "#q14\n",
    "def loader_loop(net, loader_list, epochs, loss_f, opt):\n",
    "    avg_loss = []\n",
    "    epoch_list = []\n",
    "    acc_list = []\n",
    "    for i in range(epochs):\n",
    "        print('epoch = ', i)\n",
    "        train_loss =[]\n",
    "        for loader in loader_list:\n",
    "            for j, (x, y) in enumerate(loader['train_set']):\n",
    "                opt.zero_grad()\n",
    "                x_batch = x\n",
    "                y_batch = y\n",
    "                output = net.forward(x_batch)\n",
    "                loss = loss_f(output, y_batch)\n",
    "                train_loss.append(loss.item())\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "        print('avg_loss=', sum(train_loss)/len(train_loss))\n",
    "        avg_loss.append(sum(train_loss)/len(train_loss))\n",
    "        epoch_list.append(i)\n",
    "        acc_list.append(validate(net, loader))\n",
    "    return [avg_loss, epoch_list, acc_list]\n",
    "\n",
    "def image_folder(path):\n",
    "    training_data = torchvision.datasets.ImageFolder(path+'mnist-varres/train', transform =  transforms.Compose([transforms.Grayscale(num_output_channels=1), transforms.ToTensor()]))\n",
    "    test_set= torchvision.datasets.ImageFolder(path+'mnist-varres/test', transform =  transforms.Compose([transforms.Grayscale(num_output_channels=1), transforms.ToTensor()]))\n",
    "    trainsize = round(0.8*len(training_data))\n",
    "    train_set, val_set = torch.utils.data.random_split(training_data, [trainsize, len(training_data)-trainsize]) \n",
    "    print(len(train_set), len(val_set))\n",
    "    return [train_set, val_set, test_set]\n",
    "\n",
    "def get_data(path1, path2, path3):\n",
    "    data1 = image_folder(path1)\n",
    "    data2 = image_folder(path2)\n",
    "    data3 = image_folder(path3)\n",
    "    return data1, data2, data3\n",
    "\n",
    "def get_loaders_mixed(batch_size, train_set, val_set, test_set):\n",
    "    loaders = {'train_set' : DataLoader(train_set, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "               'val_set' : DataLoader(val_set, \n",
    "                                          batch_size=len(val_set), \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "                'test_set'  : DataLoader(test_set, \n",
    "                                          batch_size=len(test_set), \n",
    "                                          shuffle=False, \n",
    "                                          num_workers=1)}\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters(model):\n",
    "    count=0\n",
    "    for param in list(model.parameters()):\n",
    "        n=1\n",
    "        for s in list(param.size()):\n",
    "            n = n*s\n",
    "        count += n\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_chan = 1\n",
    "output = 10\n",
    "kernel = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "model_fixed = NetFixed(input_chan, kernel, stride, padding, output)\n",
    "print('fixed model parameters = ', get_parameters(model_fixed))\n",
    "model_mixed = Net(input_chan, kernel, stride, padding, output, pool='avg')\n",
    "print('mixed model parameters = ', get_parameters(model_mixed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '32x32/'\n",
    "path2 = '48x48/'\n",
    "path3 = '64x64/'\n",
    "data32, data48, data64 = get_data(path1, path2, path3)\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "loaders32 = get_loaders_mixed(batch_size, data32[0], data32[1], data32[2])\n",
    "loaders48 = get_loaders_mixed(batch_size, data48[0], data48[1], data48[2])\n",
    "loaders64 = get_loaders_mixed(batch_size, data64[0], data64[1], data64[2])\n",
    "loader_list = [loaders32, loaders48, loaders64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_chan = 1\n",
    "output = 10\n",
    "kernel = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "epochs = 10\n",
    "lr = 0.0005\n",
    "\n",
    "net_max = Net(input_chan, kernel, stride, padding, output, pool='max')\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(net_max.parameters(), lr)\n",
    "results_max = loader_loop(net_max, loader_list, epochs, loss_f, opt)\n",
    "\n",
    "net_avg = Net(input_chan, kernel, stride, padding, output, pool='avg')\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(net_avg.parameters(), lr)\n",
    "results_avg = loader_loop(net_avg, loader_list, epochs, loss_f, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = results_avg[2]\n",
    "x1 = results_avg[1]\n",
    "plt.plot(x1, y1, label = 'AvgPool')\n",
    "y2 = results_max[2]\n",
    "x2 = results_max[1]\n",
    "plt.plot(x2, y2, label = 'MaxPool')\n",
    "\n",
    "plt.title('Accuracy of Avg & Max Pooling')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(color='b', linestyle='-', linewidth=0.1)\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('figures/q16_acc_avg_max')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = results_avg[0]\n",
    "x1 = results_avg[1]\n",
    "plt.plot(x1, y1, label = 'AvgPool')\n",
    "\n",
    "y2 = results_max[0]\n",
    "x2 = results_max[1]\n",
    "plt.plot(x2, y2, label = 'MaxPool')\n",
    "\n",
    "plt.title('Loss with Global Avg & Max Pooling')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(color='b', linestyle='-', linewidth=0.1)\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('figures/q16_Loss_avg_max')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, loaders, epochs, loss_f, opt):\n",
    "    avg_loss = []\n",
    "    epoch_list = []\n",
    "    acc_list = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print('epoch = ', i)\n",
    "        train_loss =[]\n",
    "        for j, (x, y) in enumerate(loaders['train_set']):\n",
    "            opt.zero_grad()\n",
    "            x_batch = x\n",
    "            y_batch = y\n",
    "            output = net.forward(x_batch)\n",
    "            loss = loss_f(output, y_batch)\n",
    "            train_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            opt.step() \n",
    "        print('avg_loss=', sum(train_loss)/len(train_loss))\n",
    "        avg_loss.append(sum(train_loss)/len(train_loss))\n",
    "        epoch_list.append(i)\n",
    "        acc_list.append(validate(net, loaders))\n",
    "    return [avg_loss, epoch_list, acc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make loaders of the one size data\n",
    "def get_loaders_onesize(batch_size, train_set, val_set, test_set):\n",
    "    loaders = {'train_set' : DataLoader(train_set, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "               'val_set' : DataLoader(val_set, \n",
    "                                          batch_size=len(val_set), \n",
    "                                          shuffle=True, \n",
    "                                          num_workers=1),\n",
    "                'test_set'  : DataLoader(test_set, \n",
    "                                          batch_size=len(test_set), \n",
    "                                          shuffle=False, \n",
    "                                          num_workers=1)}\n",
    "    return loaders\n",
    "\n",
    "one_size = transforms.Compose(\n",
    "                    [\n",
    "                    transforms.Resize((28,28)),\n",
    "                    transforms.Grayscale(num_output_channels=1),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.1308,), (0.3016,)) \n",
    "                    ])\n",
    "\n",
    "training_data_fz = torchvision.datasets.ImageFolder('mnist-varres/train', transform=one_size)\n",
    "test_set_fz= torchvision.datasets.ImageFolder('mnist-varres/test', transform=one_size)\n",
    "train_set_fz, val_set_fz = torch.utils.data.random_split(training_data_fz, [50000, 10000]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate loops\n",
    "def lr_loop(input_chan, kernel, stride, padding, output, loader_list, epochs):\n",
    "    results = []\n",
    "    lr = 0.0005\n",
    "    for i in range(10):\n",
    "        net = Net(input_chan, kernel, stride, padding, output, pool='max')\n",
    "        loss_f = nn.CrossEntropyLoss()\n",
    "        opt = optim.Adam(net.parameters(), lr)\n",
    "        results.append(loader_loop(net, loader_list, epochs, loss_f, opt))\n",
    "        lr += 0.0005\n",
    "    return results\n",
    "\n",
    "def lr_loop_fixed(train_set_fz, val_set_fz, test_set_fz, epochs, batch_size, input_chan, kernel, stride, padding, output):\n",
    "    results = []\n",
    "    lr = 0.0005\n",
    "    for i in range(10):\n",
    "        net = NetFixed(input_chan, kernel, stride, padding, output,)\n",
    "        loss_f = nn.CrossEntropyLoss()\n",
    "        opt = optim.Adam(net.parameters(), lr)\n",
    "        loaders = get_loaders_onesize(batch_size, train_set_fz, val_set_fz, test_set_fz,)\n",
    "        results.append(train(net, loaders, epochs, loss_f, opt))\n",
    "        lr += 0.0005\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_chan = 1 \n",
    "output = 10\n",
    "kernel = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "epochs = 5\n",
    "batch_size = 16\n",
    "\n",
    "lrResults = lr_loop(input_chan, kernel, stride, padding, output, loader_list, epochs)\n",
    "lrResultsFixed = lr_loop_fixed(train_set_fz, val_set_fz, test_set_fz, epochs, batch_size, input_chan, kernel, stride, padding, output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot learning rates\n",
    "def plot_save_lr_results(lr_results, name, title):\n",
    "    plt.figure(figsize=(8, 6), dpi=80)\n",
    "    y1 = lr_results[0][2]\n",
    "    x1 = lr_results[0][1]\n",
    "    plt.plot(x1, y1, label = '0.0005')\n",
    "\n",
    "    y1 = lr_results[1][2]\n",
    "    x1 = lr_results[1][1]\n",
    "    plt.plot(x1, y1, label = '0.0010')\n",
    "\n",
    "    y1 = lr_results[2][2]\n",
    "    x1 = lr_results[2][1]\n",
    "    plt.plot(x1, y1, label = '0.0015')\n",
    "\n",
    "    y1 = lr_results[3][2]\n",
    "    x1 = lr_results[3][1]\n",
    "    plt.plot(x1, y1, label = '0.0020')\n",
    "\n",
    "    y1 = lr_results[4][2]\n",
    "    x1 = lr_results[4][1]\n",
    "    plt.plot(x1, y1, label = '0.0025')\n",
    "\n",
    "    y1 = lr_results[5][2]\n",
    "    x1 = lr_results[5][1]\n",
    "    plt.plot(x1, y1, label = '0.0030')\n",
    "\n",
    "    y1 = lr_results[6][2]\n",
    "    x1 = lr_results[6][1]\n",
    "    plt.plot(x1, y1, label = '0.0035')\n",
    "\n",
    "    y1 = lr_results[7][2]\n",
    "    x1 = lr_results[7][1]\n",
    "    plt.plot(x1, y1, label = '0.0040')\n",
    "\n",
    "    y1 = lr_results[8][2]\n",
    "    x1 = lr_results[8][1]\n",
    "    plt.plot(x1, y1, label = '0.0045')\n",
    "\n",
    "    y1 = lr_results[9][2]\n",
    "    x1 = lr_results[9][1]\n",
    "    plt.plot(x1, y1, label = '0.0050')\n",
    "\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(title)\n",
    "    plt.grid(color='b', linestyle='-', linewidth=0.1)\n",
    "    plt.xticks(np.arange(0, 5, 1))\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig('figures/q17_' + name)\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_save_lr_results(lrResults, 'tune_lr_maxpool', 'MaxPool network: Accuracies with learning rates')\n",
    "plot_save_lr_results(lrResultsFixed, 'tune_lr_maxpool', 'Reshape Network: Accuracies with learning rates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bs_loop(input_chan, kernel, stride, padding, output, superloader, epochs):\n",
    "    results = []\n",
    "    lr = 0.0010\n",
    "    for loader_list in superloader:\n",
    "        net = Net(input_chan, kernel, stride, padding, output, pool='max')\n",
    "        loss_f = nn.CrossEntropyLoss()\n",
    "        opt = optim.Adam(net.parameters(), lr)\n",
    "        results.append(loader_loop(net, loader_list, epochs, loss_f, opt))\n",
    "    return results\n",
    "\n",
    "def bs_loop_fixed(train_set_fz, val_set_fz, test_set_fz, epochs, batch_size, input_chan, kernel, stride, padding, output):\n",
    "    results = []\n",
    "    lr = 0.0015\n",
    "    for batch_size in bs_list:\n",
    "        net = NetFixed(input_chan, kernel, stride, padding, output,)\n",
    "        loss_f = nn.CrossEntropyLoss()\n",
    "        opt = optim.Adam(net.parameters(), lr)\n",
    "        loaders = get_loaders_onesize(batch_size, train_set_fz, val_set_fz, test_set_fz,)\n",
    "        results.append(train(net, loaders, epochs, loss_f, opt))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader_list(batch_size, data32, data48, data64):\n",
    "    loaders32 = get_loaders_mixed(batch_size, data32[0], data32[1], data32[2])\n",
    "    loaders48 = get_loaders_mixed(batch_size, data48[0], data48[1], data48[2])\n",
    "    loaders64 = get_loaders_mixed(batch_size, data64[0], data64[1], data64[2])\n",
    "    loader_list = [loaders32, loaders48, loaders64]\n",
    "    return loader_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_chan = 1 \n",
    "output = 10\n",
    "kernel = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "bs_list = [4, 16, 32, 64]\n",
    "superloader = []\n",
    "for i in bs_list:\n",
    "    superloader.append(get_loader_list(i, data32, data48, data64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsResultsFixed = bs_loop_fixed(train_set_fz, val_set_fz, test_set_fz, epochs, batch_size, input_chan, kernel, stride, padding, output)\n",
    "bsResults = bs_loop(input_chan, kernel, stride, padding, output, superloader, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_value(inputlist):\n",
    "    return max([sublist[-1] for sublist in inputlist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_acc = max_value(bsResults)\n",
    "max_acc_fixed = max_value(bsResults)\n",
    "\n",
    "print(\"best batch sizes for the networks are:\")\n",
    "print(bs_list.index(max_acc))\n",
    "print(bs_list.index(max_acc_fixed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training with tuned params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '32x32/'\n",
    "path2 = '48x48/'\n",
    "path3 = '64x64/'\n",
    "data32, data48, data64 = get_data(path1, path2, path3)\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "loaders32 = get_loaders_mixed(batch_size, data32[0], data32[1], data32[2])\n",
    "loaders48 = get_loaders_mixed(batch_size, data48[0], data48[1], data48[2])\n",
    "loaders64 = get_loaders_mixed(batch_size, data64[0], data64[1], data64[2])\n",
    "loader_list = [loaders32, loaders48, loaders64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, loader):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader['test_set']: \n",
    "            output = net(x)\n",
    "            _,pred_y = torch.max(output, dim = 1)\n",
    "            correct = np.array(pred_y == y)  \n",
    "            accuracy = np.count_nonzero(correct)/len(correct)\n",
    "        print(\"Acc = \", accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_chan = 1\n",
    "output = 10\n",
    "kernel = 3\n",
    "stride = 1\n",
    "padding = 1\n",
    "epochs = 20  #adjust to optimal epochs for final training\n",
    "lr = 0.0010\n",
    "\n",
    "netMax = Net(input_chan, kernel, stride, padding, output, pool='max')\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(netMax.parameters(), lr)\n",
    "results_max = loader_loop(netMax, loader_list, epochs, loss_f, opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "test(netMax, random.choice(loader_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0015\n",
    "epochs = 20 #adjust to optimal epochs for final training\n",
    "batch_size = 16\n",
    "loadersb16 = get_loaders_onesize(batch_size, train_set_fz, val_set_fz, test_set_fz,)\n",
    "netFixed = NetFixed(input_chan, kernel, stride, padding, output)\n",
    "loss_f = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(netFixed.parameters(), lr)\n",
    "results_fixed = train(netFixed, loadersb16, epochs, loss_f, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(netFixed, loadersb16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = results_fixed[2]\n",
    "x1 = results_fixed[1]\n",
    "plt.plot(x1, y1, label = 'Reshape Network')\n",
    "\n",
    "y2 = results_max[2]\n",
    "x2 = results_max[1]\n",
    "plt.plot(x2, y2, label = 'MaxPool Network')\n",
    "\n",
    "plt.title('Accuracy during training epochs')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(np.arange(0, 21, 1))\n",
    "plt.grid(color='b', linestyle='-', linewidth=0.1)\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('figures/q17_2networks_acc')\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e2d738271302b869834cae786a1c397433e224a04daa139e6e226c81326f323"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
