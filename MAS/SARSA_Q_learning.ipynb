{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA & Q-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Multi Agent systems - Assignment 6\n",
    "Tiddo Loos\n",
    "date:08-01-2020\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np;np.random.seed(0)\n",
    "import seaborn as sns;sns.set_theme()\n",
    "import random\n",
    "import operator\n",
    "\n",
    "\n",
    "class State:\n",
    "    def __init__(self, y, x, grid, wall=False, treasure=False, snakepit=False):\n",
    "        self.y = y\n",
    "        self.x = x\n",
    "        self.grid = grid\n",
    "        self.neighbours = []\n",
    "        self.value = 0\n",
    "        self.wall = wall\n",
    "        self.treasure = treasure\n",
    "        self.snakepit = snakepit\n",
    "\n",
    "    def get_neighbours(self, state):\n",
    "        if state.y >= 1:\n",
    "            self.neighbours.append(self.grid.states[self.y - 1][self.x])\n",
    "        if state.y <= self.grid.size - 2:\n",
    "            self.neighbours.append(self.grid.states[self.y + 1][self.x])\n",
    "        if state.x >= 1:\n",
    "            self.neighbours.append(self.grid.states[self.y][self.x - 1])\n",
    "        if state.x <= self.grid.size - 2:\n",
    "            self.neighbours.append(self.grid.states[self.y][self.x + 1])\n",
    "        for neighbour in self.neighbours:\n",
    "            if neighbour.wall:\n",
    "                self.neighbours.remove(neighbour)\n",
    "        add_current_state = 4 - len(self.neighbours)\n",
    "        for i in range(add_current_state):\n",
    "            self.neighbours.append(self.grid.states[self.y][self.x])\n",
    "\n",
    "    def update_state_value(self):\n",
    "        new_value = 0\n",
    "        if self.grid.states[self.y][self.x].wall:\n",
    "            self.value = 0\n",
    "        elif self.grid.states[self.y][self.x].treasure:\n",
    "            self.value = self.grid.reward_treasure\n",
    "        elif self.grid.states[self.y][self.x].snakepit:\n",
    "            self.value = self.grid.reward_snakepit\n",
    "        else:\n",
    "            for neighbour in self.neighbours:\n",
    "                reward = self.get_reward_state(neighbour)\n",
    "                new_value += 1/4 * reward\n",
    "            self.value = new_value\n",
    "\n",
    "    def get_reward_state(self, neighbour):\n",
    "        y, x = neighbour.y, neighbour.x\n",
    "        if (y, x) is self.grid.treasure:\n",
    "            reward = self.grid.reward_treasure\n",
    "        elif (y, x) is self.grid.snakepit:\n",
    "            reward = self.grid.reward_snakepit\n",
    "        else:\n",
    "            reward = (self.grid.cost + neighbour.value)\n",
    "        return reward\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, pos_y, pos_x, grid):\n",
    "        self.pos_y = pos_y\n",
    "        self.pos_x = pos_x\n",
    "        self.agent_reward = 0\n",
    "        self.reward_list = []\n",
    "        self.grid = grid\n",
    "\n",
    "    def walk_agent(self):\n",
    "        action_one = self.e_greedy(self.grid.q_value)\n",
    "        while ([(self.pos_y, self.pos_x)]) != self.grid.treasure and ([(self.pos_y, self.pos_x)]) != self.grid.snakepit:\n",
    "            [current_y, current_x] = self.pos_y, self.pos_x\n",
    "            self.action(self.pos_y, self.pos_x, action_one, self.grid.walls, self.grid.size)\n",
    "            reward = self.grid.grid_reward(self.pos_y, self.pos_x)\n",
    "            self.agent_reward += reward\n",
    "            new_action = self.e_greedy(self.grid.q_value)\n",
    "            [new_y, new_x] = self.pos_y, self.pos_x\n",
    "            # sarsa or q learning\n",
    "            if self.grid.sarsa:\n",
    "                update_action = new_action\n",
    "            else:\n",
    "                update_action = self.e_greedy(self.grid.q_value)\n",
    "            self.grid.update_q_values(current_y, current_x, action_one, reward, new_y, new_x, update_action)\n",
    "            action_one = new_action\n",
    "        return\n",
    "\n",
    "    def e_greedy(self, q_value):\n",
    "        global count\n",
    "        if self.grid.sarsa:\n",
    "            e = 0.1\n",
    "        else:\n",
    "            e = 0 #q_learning\n",
    "\n",
    "        [y, x] = [self.pos_y, self.pos_x]\n",
    "        if np.random.uniform(0,1) < e:\n",
    "            return np.random.choice(list(q_value[y][x].keys()))\n",
    "        else:\n",
    "            return max(q_value[y][x], key=q_value[y][x].get)\n",
    "\n",
    "    def action(self, y, x, action, walls, size):\n",
    "        current_state = [y, x]\n",
    "        if action == 'north':\n",
    "            next_state = (current_state[0]-1, current_state[1])\n",
    "        if action == 'south':\n",
    "            next_state = (current_state[0]+1, current_state[1])\n",
    "        if action == 'east':\n",
    "            next_state = (current_state[0], current_state[1]+1)\n",
    "        if action == 'west':\n",
    "            next_state = (current_state[0], current_state[1]-1)\n",
    "        self.update_position(current_state, next_state, walls, size)\n",
    "\n",
    "    def update_position(self, current_state, next_state, walls, size):\n",
    "        if next_state in walls or \\\n",
    "                 next_state[0] < 0 or \\\n",
    "                 next_state[0] >= size or \\\n",
    "                 next_state[1] < 0 or \\\n",
    "                 next_state[1] >= size:\n",
    "            new_pos = current_state\n",
    "        else:\n",
    "            new_pos = next_state\n",
    "        self.pos_y = new_pos[0]\n",
    "        self.pos_x = new_pos[1]\n",
    "\n",
    "\n",
    "class Grid:\n",
    "    def __init__(self, walls, treasure, snakepit, size, reward_treasure, reward_snakepit, cost, alfa, gamma, sarsa=False):\n",
    "        self.states = []\n",
    "        self.walls = walls\n",
    "        self.treasure = treasure\n",
    "        self.snakepit = snakepit\n",
    "        self.size = size\n",
    "        self.reward_snakepit = reward_snakepit\n",
    "        self.reward_treasure = reward_treasure\n",
    "        self.cost = cost\n",
    "        self.values_list = []\n",
    "        self.init_grid()\n",
    "        # assignment 2: sarsa/q_learning\n",
    "        self.alfa = alfa\n",
    "        self.gamma = gamma\n",
    "        self.sarsa = sarsa\n",
    "        self.q_value = [[{'north': 0., 'south': 0., 'east': 0., 'west': 0.}for _ in range(self.size)]for _ in range(self.size)]\n",
    "        self.pos_y = []\n",
    "        self.pos_x = []\n",
    "        self.policy_y = []\n",
    "        self.policy_sarsa_x = []\n",
    "        self.create_agent_on_grid()\n",
    "\n",
    "    def init_grid(self):\n",
    "        self.fill_grid()\n",
    "        for row in self.states:\n",
    "            for state in row:\n",
    "                state.get_neighbours(state)\n",
    "        for i in range(500):\n",
    "            self.get_grid_values()\n",
    "\n",
    "    def fill_grid(self):\n",
    "        for y in range(self.size):\n",
    "            row = []\n",
    "            for x in range(self.size):\n",
    "                if (y, x) in self.walls:\n",
    "                    row.append(State(y, x, self, wall=True))\n",
    "                elif (y, x) in self.treasure:\n",
    "                    row.append(State(y, x, self, treasure=True))\n",
    "                elif (y, x) in self.snakepit:\n",
    "                    row.append(State(y, x, self, snakepit=True))\n",
    "                else:\n",
    "                    row.append(State(y, x, self))\n",
    "            self.states.append(row)\n",
    "\n",
    "    def get_grid_values(self):\n",
    "        for row in self.states:\n",
    "            for state in row:\n",
    "                state.update_state_value()\n",
    "\n",
    "    def append_value_list(self):\n",
    "        for row in self.states:\n",
    "            row_values = []\n",
    "            for state in row:\n",
    "                row_values.append(state.value)\n",
    "            self.values_list.append(row_values)\n",
    "\n",
    "    def show_heatmap(self):\n",
    "        self.append_value_list()\n",
    "        sns.heatmap(self.values_list, annot=True, fmt='0.1f')\n",
    "        plt.show()\n",
    "\n",
    "    def create_agent_on_grid(self):\n",
    "        global count\n",
    "        count = 0\n",
    "        for i in range(100):\n",
    "            y, x = self.random_position()\n",
    "            Agent(y, x, self).walk_agent()\n",
    "            count += 1\n",
    "        self.save_policy_data(self.policy_y, self.policy_sarsa_x)\n",
    "\n",
    "    def random_position(self):\n",
    "        y, x = self.walls[0]\n",
    "        while (y, x) in self.walls:\n",
    "            y = random.randint(0, self.size-1)\n",
    "            x = random.randint(0, self.size-1)\n",
    "        position = (y, x)\n",
    "        # position = (0, 0) #to start always form the 0,0 position\n",
    "        return position\n",
    "\n",
    "    def grid_reward(self, y, x):\n",
    "        current_pos = (y, x)\n",
    "        if [current_pos] == self.snakepit:\n",
    "            return self.reward_snakepit\n",
    "        elif [current_pos] == self.treasure:\n",
    "            return self.reward_treasure\n",
    "        else:\n",
    "            return self.cost\n",
    "\n",
    "    def update_q_values(self, current_y, current_x, action, reward, new_y, new_x, update_action):\n",
    "        self.q_value[current_y][current_x][action] += self.alfa * (reward + self.gamma * self.q_value[new_y][new_x][update_action]-self.q_value[current_y][current_x][action])\n",
    "        return\n",
    "\n",
    "    def save_policy_data(self, policy_y, policy_x):\n",
    "        for y in range(len(self.q_value)):\n",
    "            for x in range(len(self.q_value[y])):\n",
    "                direction = max(self.q_value[y][x].items(), key=operator.itemgetter(1))\n",
    "                if direction[1] == 0.0:\n",
    "                    policy_y.append(0)\n",
    "                    policy_x.append(0)\n",
    "                elif direction[0] == 'north':\n",
    "                    policy_y.append(0.3)\n",
    "                    policy_x.append(0)\n",
    "                elif direction[0] == 'east':\n",
    "                    policy_y.append(0)\n",
    "                    policy_x.append(0.3)\n",
    "                elif direction[0] == 'west':\n",
    "                    policy_y.append(0)\n",
    "                    policy_x.append(-0.3)\n",
    "                else:\n",
    "                    policy_y.append(-0.3)\n",
    "                    policy_x.append(0)\n",
    "        return\n",
    "\n",
    "    def show_quiver_map(self):\n",
    "        pos_y = []\n",
    "        pos_x = []\n",
    "        for y in range(len(self.q_value)):\n",
    "            for x in range(len(self.q_value[y])):\n",
    "                pos_y.append(y)\n",
    "                pos_x.append(x)\n",
    "        pos_y_r = pos_y[-1::-1]\n",
    "        fig, ax = plt.subplots()\n",
    "        if self.sarsa:\n",
    "            title = 'SARSA policies (e=0.1)'\n",
    "        else:\n",
    "            title = 'Q-learning policies (e=0)'\n",
    "        plt.title(title)\n",
    "        ax.quiver(pos_x, pos_y_r, self.policy_sarsa_x, self.policy_y, scale=5)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    walls = [(7, 1), (7, 2), (7, 3), (7, 4), (5, 6), (4, 6), (3, 6), (2, 6), (1, 6), (1, 5), (1, 4), (1, 3), (1, 2)]\n",
    "    treasure = [(8, 8)]\n",
    "    snakepit = [(6, 5)]\n",
    "    size = 9\n",
    "    reward_snakepit = -50\n",
    "    reward_treasure = 50\n",
    "    cost = -1\n",
    "\n",
    "    # to use the sarsa algorithm set sarsa=True)\n",
    "    grid = Grid(walls, treasure, snakepit, size, reward_treasure, reward_snakepit, cost, alfa=0.5, gamma=1, sarsa=True)\n",
    "    grid.show_heatmap()\n",
    "    \n",
    "    grid.show_quiver_map()\n",
    "\n",
    "    # create new grid (set q values to 0) and use Q-learning\n",
    "    grid_q = Grid(walls, treasure, snakepit, size, reward_treasure, reward_snakepit, cost, alfa=0.5, gamma=1)\n",
    "    grid_q.show_quiver_map()\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
